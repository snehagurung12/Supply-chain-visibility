# üß† Machine Learning ‚Äî Supply Chain Visibility & Predictive Analysis

This folder contains all machine learning-related assets for the project.  
It includes model training scripts, exported predictions, evaluation metrics, and plots used in the Power BI **Forecast** and **Delay Risk** pages.

---

## üìÅ Folder Structure

ML/

‚îú‚îÄ notebooks/ # development & training

‚îÇ ‚îú‚îÄ 01_features_delay.ipynb

‚îÇ ‚îî‚îÄ 02_forecast_demand.ipynb

‚îú‚îÄ models/ # optional: saved models / model cards

‚îú‚îÄ outputs/ # ‚¨Ö Power BI reads these

‚îÇ ‚îú‚îÄ shipment_delay_pred.csv

‚îÇ ‚îî‚îÄ forecast_fact.csv

‚îú‚îÄ metrics/ # evaluation artifacts (JSON/CSV)

‚îÇ ‚îú‚îÄ delay_regression.json # {rmse, mae, r2}

‚îÇ ‚îî‚îÄ forecast_metrics.json # {mape, rmse, r2}

‚îú‚îÄ visuals/ # charts exported from notebooks

‚îî‚îÄ README.md



---

## üéØ Objectives

- **Delay regression:** predict `DelayDays` for each shipment using route, distance, supplier lead time, etc.
- **Demand forecast:** predict upcoming order volume at month/region/category granularity.

---

## üîÑ Workflow

Data/clean ‚Üí ML/notebooks (feature, train, eval) ‚Üí ML/outputs/*.csv ‚Üí Power BI

Inputs expected:
- `Data/clean/fact_shipments.csv`
- `Data/clean/fact_orders.csv`
- `Data/clean/dim_suppliers.csv`
- `Data/clean/dim_calendar.csv`

Outputs produced:
- `ML/outputs/shipment_delay_pred.csv`
- `ML/outputs/forecast_fact.csv`

---

## üßÆ Models & Metrics

| Task | Model | Key Features | Target | Metrics (saved in /metrics) | Output |
|---|---|---|---|---|---|
| Delay Prediction | RandomForestRegressor | DistanceKM, LeadTimeDays, SupplierRating, Route, Month | DelayDays | `rmse`, `mae`, `r2` ‚Üí `delay_regression.json` | `shipment_delay_pred.csv` |
| Demand Forecast | (your choice) e.g., XGBoost / DT / SARIMAX | Month, Region, Category, Lag features | Next-period Orders | `rmse`, `mape`, `r2` ‚Üí `forecast_metrics.json` | `forecast_fact.csv` |

> **Tip:** Use calendar joins to add `Month`, `Year`, `Season`, and lag features.

---

## üß™ Minimal example (pseudo-code)

```python
import pandas as pd, numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

ship = pd.read_csv('Data/clean/fact_shipments.csv')
X = ship[['DistanceKM','LeadTimeDays','SupplierRating']].fillna(0)
y = ship['DelayDays'].fillna(0)

rf = RandomForestRegressor(n_estimators=200, random_state=42)
rf.fit(X, y)

pred = rf.predict(X)
rmse = mean_squared_error(y, pred, squared=False)
mae  = mean_absolute_error(y, pred)
r2   = r2_score(y, pred)

pd.DataFrame({
    'ShipmentID': ship['ShipmentID'],
    'DelayDays': y,
    'PredictedDelay': pred
}).to_csv('ML/outputs/shipment_delay_pred.csv', index=False)

pd.Series({'rmse': rmse, 'mae': mae, 'r2': r2}).to_json('ML/metrics/delay_regression.json')
üîó Power BI Integration

Operations / Risk pages ‚Üí ML/outputs/shipment_delay_pred.csv

Forecast page ‚Üí ML/outputs/forecast_fact.csv (columns: Date, Actual, Forecast, optional Lower, Upper)

After you run notebooks, open the PBIX:

Transform Data ‚Üí Data source settings ‚Üí Change Source to ML/outputs/‚Ä¶ (once).

Refresh ‚Üí visuals update automatically.

‚òÅÔ∏è Azure Upgrade (later)
| Function           | Azure service                    |
| ------------------ | -------------------------------- |
| Notebook execution | Azure ML / Synapse Spark         |
| Model registry     | Azure ML Registry                |
| Batch scoring      | ADF Pipeline (notebook activity) |
| Output storage     | Blob `ml-outputs/`               |
| BI                 | Power BI Service / Fabric        |

üìú Notes

Synthetic / anonymized data only; for academic/portfolio use.

Keep random seeds fixed for reproducibility.

If outputs grow large, track with Git LFS.

---

# üìÑ `ML/models/README.md` ‚Äî (optional) copy-paste

```markdown
# models/

Optional folder for persisted models and model cards.

- Store `.pkl` files only if needed.
- Prefer rebuilding models from notebooks for transparency.
- Include a short **MODEL_CARD.md** per model (data, features, metrics, caveats).
