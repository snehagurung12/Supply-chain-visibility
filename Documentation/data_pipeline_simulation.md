# ðŸ§® Data Pipeline Simulation (ETL Flow)


```python
Step 1 â€” Ingestion
orders = pd.read_csv('Data/raw/orders.csv')
shipments = pd.read_csv('Data/raw/shipments.csv')
Step 2 â€” Cleaning

Remove duplicates, fix date columns, handle missing supplier data.

Step 3 â€” Transformation

Create new calculated fields:

LateFlag

DelayDays

OnTime% summary

Step 4 â€” Merge

Join Orders, Shipments, Suppliers â†’ Unified dataset.

Step 5 â€” Output

Export processed_data.csv for ML & Power BI.

ðŸ“˜ Equivalent to: Azure Data Factory pipeline â†’ Data Flow â†’ Sink.

---

## ðŸ“ˆ 12. `processed_data.csv`
âœ… Keep as a small cleaned version (show structure).  
Optional: Create a table in the README showing the first 5 rows (sample data).

---

## ðŸ“† 13. `progress_week2.txt`
Replace it with a formatted log:

